{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916bdb33",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac051501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#read file.\n",
    "df1 = pd.read_csv('Ultimate_AAPL_filtered.csv')\n",
    "df2 = pd.read_csv('Ultimate_AMZN_filtered.csv')\n",
    "df3 = pd.read_csv('Ultimate_GOOGL_filtered.csv')\n",
    "df4 = pd.read_csv('Ultimate_MSFT_filtered.csv')\n",
    "df5 = pd.read_csv('Ultimate_TSLA_filtered.csv')\n",
    "\n",
    "# combine all companies' data\n",
    "df = pd.DataFrame()\n",
    "df = df.append(df1, ignore_index=True)\n",
    "df = df.append(df2, ignore_index=True)\n",
    "df = df.append(df3, ignore_index=True)\n",
    "df = df.append(df4, ignore_index=True)\n",
    "df = df.append(df5, ignore_index=True)\n",
    "df= df.drop(columns=['post_date','post_date_date'])\n",
    "df['Increase'] = (df['Close_price-2days'] > df['Close_price-tmr'])\n",
    "X=df[[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "Y=df['Increase']\n",
    "\n",
    "    \n",
    "# normalize variables\n",
    "scaler = MinMaxScaler()\n",
    "scaledX = scaler.fit_transform(X)\n",
    "scaled_df = pd.DataFrame(scaledX,columns=[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"])\n",
    "scaled_df['Increase'] = Y\n",
    "newX=scaled_df[[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "newY=scaled_df['Increase']\n",
    "\n",
    "# split training data and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(newX, newY, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_list = [1,10,100,1000]\n",
    "scores_linear = []\n",
    "scores_poly = []\n",
    "scores_gauss = []\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "for i in range(len(C_list)):\n",
    "    svm_linear = SVC(kernel=\"linear\", C=C_list[i])\n",
    "    svm_poly = SVC(kernel='poly', degree=2, C=C_list[i])  #polynomial kernel with degree 2.\n",
    "    svm_gauss = SVC(kernel='rbf', gamma=1, C=C_list[i])   #gaussian rbf kernel\n",
    "    \n",
    "    #train the model with train data set\n",
    "    svm_linear.fit(X_train,y_train) \n",
    "    svm_poly.fit(X_train,y_train)\n",
    "    svm_gauss.fit(X_train,y_train)\n",
    "    \n",
    "    #test the model with test data set and see how close are the prediction from real\n",
    "    scores_linear.append(svm_linear.score(X_test,y_test))\n",
    "    scores_poly.append(svm_poly.score(X_test,y_test))\n",
    "    scores_gauss.append(svm_gauss.score(X_test,y_test))\n",
    "\n",
    "scores = pd.DataFrame([scores_linear,scores_poly,scores_gauss], index = ['linear','poly','gauss'], columns = C_list)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af05aa91",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da28df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#read file.\n",
    "df1 = pd.read_csv('Ultimate_AAPL_filtered.csv')\n",
    "df2 = pd.read_csv('Ultimate_AMZN_filtered.csv')\n",
    "df3 = pd.read_csv('Ultimate_GOOGL_filtered.csv')\n",
    "df4 = pd.read_csv('Ultimate_MSFT_filtered.csv')\n",
    "df5 = pd.read_csv('Ultimate_TSLA_filtered.csv')\n",
    "\n",
    "# combine all companies' data\n",
    "df = pd.DataFrame()\n",
    "df = df.append(df1, ignore_index=True)\n",
    "df = df.append(df2, ignore_index=True)\n",
    "df = df.append(df3, ignore_index=True)\n",
    "df = df.append(df4, ignore_index=True)\n",
    "df = df.append(df5, ignore_index=True)\n",
    "df= df.drop(columns=['post_date','post_date_date'])\n",
    "df['Increase'] = (df['Close_price-2days'] > df['Close_price-tmr'])\n",
    "X=df[[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "Y=df['Increase']\n",
    "    \n",
    "# normalize variables\n",
    "scaler = MinMaxScaler()\n",
    "scaledX = scaler.fit_transform(X)\n",
    "scaled_df = pd.DataFrame(scaledX,columns=[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"])\n",
    "scaled_df['Increase'] = Y\n",
    "newX=scaled_df[[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "newY=scaled_df['Increase']\n",
    "    \n",
    "# split training data, validation data, testing data\n",
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(newX, newY, test_size=0.3)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ef94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "#  input shape is 7 since we have 7 input features as indepent variables # all 32 units share the training size\n",
    "# meaning that each unit has size (training_size)/32\n",
    "model = Sequential([\n",
    "    Dense(units = 32, activation='relu', input_shape=(7,)),\n",
    "    Dense(units = 32, activation='relu'),\n",
    "    Dense(units = 1, activation='sigmoid')\n",
    "])\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "#model.compile(loss=BinaryCrossentropy())\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=32, epochs=5, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ab110",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85bbce",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb98672d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#read file.\n",
    "df1 = pd.read_csv('Ultimate_AAPL_filtered.csv')\n",
    "df2 = pd.read_csv('Ultimate_AMZN_filtered.csv')\n",
    "df3 = pd.read_csv('Ultimate_GOOGL_filtered.csv')\n",
    "df4 = pd.read_csv('Ultimate_MSFT_filtered.csv')\n",
    "df5 = pd.read_csv('Ultimate_TSLA_filtered.csv')\n",
    "\n",
    "# combine all companies' data\n",
    "df = pd.DataFrame()\n",
    "df = df.append(df1, ignore_index=True)\n",
    "df = df.append(df2, ignore_index=True)\n",
    "df = df.append(df3, ignore_index=True)\n",
    "df = df.append(df4, ignore_index=True)\n",
    "df = df.append(df5, ignore_index=True)\n",
    "df= df.drop(columns=['post_date','post_date_date'])\n",
    "df['Increase'] = (df['Close_price-2days'] > df['Close_price-tmr'])\n",
    "X=df[[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "Y=df['Increase']\n",
    "    \n",
    "# normalize variables\n",
    "scaler = MinMaxScaler()\n",
    "scaledX = scaler.fit_transform(X)\n",
    "scaled_df = pd.DataFrame(scaledX,columns=[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"])\n",
    "scaled_df['Increase'] = Y\n",
    "newX=scaled_df[[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "newY=scaled_df['Increase']\n",
    "\n",
    "# split training data and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(newX, newY, test_size = 0.30)\n",
    "\n",
    "# set up the logistic regression from sklearn\n",
    "clf = LogisticRegression(random_state=0) # set parameter C=1e12 to make the logistic reg without regulation\n",
    "# train the model with training data X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# generate coefficients' estimations for each variable\n",
    "estimations = pd.DataFrame(clf.coef_.transpose(), [\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]).transpose()\n",
    "estimations = estimations.rename(index = {0:'coeff_estimates'})\n",
    "estimations['intercept'] = clf.intercept_\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "estimations['Accuracy'] = clf.score(X_test,y_test)\n",
    "estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492abd2d",
   "metadata": {},
   "source": [
    "# LSTM: train on Apple, test on Google"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc530f7",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63758a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df1 = pd.read_csv('Ultimate_AAPL_filtered.csv')\n",
    "df3 = pd.read_csv('Ultimate_GOOGL_filtered.csv')\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "plt.title('Stock Prices History for Apple')\n",
    "plt.plot(df1['Close_price-2days'])\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Prices ($)')\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "plt.title('Stock Prices History for Google')\n",
    "plt.plot(df3['Close_price-2days'])\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Prices ($)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d7064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do LSTM on Google in case of the time\n",
    "X=df3[[\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "Y=df3['Close_price-2days']\n",
    "    \n",
    "# normalize variables\n",
    "scaler = MinMaxScaler()\n",
    "scaledX = scaler.fit_transform(X)\n",
    "scaled_df = pd.DataFrame(scaledX,columns=[\"Close_price-today\",\"Close_price-tmr\"])\n",
    "scaled_df['Close_price-2days'] = Y\n",
    "X_train=scaled_df[[\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "Y_train=scaled_df['Close_price-2days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6941a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the LSTM model\n",
    "model = keras.Sequential()\n",
    "model.add(layers.LSTM(100, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(layers.LSTM(100, return_sequences=False))\n",
    "model.add(layers.Dense(25))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "# train the LSTM model using Google's stock\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, Y_train, batch_size= 1, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda8bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on Apple's stock\n",
    "Apple_X=df1[[\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "Apple_Y=df1['Close_price-2days']\n",
    "    \n",
    "# normalize variables\n",
    "scaler = MinMaxScaler()\n",
    "scaledX_Apple = scaler.fit_transform(Apple_X)\n",
    "scaled_df_Apple = pd.DataFrame(scaledX_Apple,columns=[\"Close_price-today\",\"Close_price-tmr\"])\n",
    "scaled_df_Apple['Close_price-2days'] = Apple_Y\n",
    "testX_Apple=scaled_df_Apple[[\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "testY_Apple=scaled_df_Apple['Close_price-2days']\n",
    "\n",
    "predictions = model.predict(testX_Apple)\n",
    "predictions = np.reshape(predictions, (predictions.shape[0])) # get rid of 2nd dimension\n",
    "error = np.sqrt(np.mean(predictions - testY_Apple)**2)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd8c34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Stock Price for Google')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Close Price USD ($)')\n",
    "plt.plot(testY_Apple)\n",
    "plt.plot(predictions)\n",
    "plt.legend(['Real', 'Predict'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282cd23c",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "filepaths = [   \"Ultimate_AAPL_filtered.csv\",    \n",
    "                \"Ultimate_TSLA_filtered.csv\",    \n",
    "                \"Ultimate_MSFT_filtered.csv\",    \n",
    "                \"Ultimate_GOOG_filtered.csv\",    \n",
    "                \"Ultimate_AMZN_filtered.csv\",]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Train the model using the first four datasets\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for filepath in filepaths[:-1]:\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    X = df[[\"comment_num\", \"retweet_num\", \"like_num\", \"follower_count\", \"compound_score\", \"Close_price-today\", \"Close_price-tmr\"]]\n",
    "    y = df['Close_price-2days']\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_train_list.append(X_scaled)\n",
    "    y_train_list.append(y.values)\n",
    "\n",
    "X_train = np.vstack(X_train_list)\n",
    "y_train = np.concatenate(y_train_list)\n",
    "\n",
    "knn_reg.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the fifth dataset\n",
    "df_test = pd.read_csv(filepaths[-1])\n",
    "\n",
    "X_test = df_test[[\"comment_num\", \"retweet_num\", \"like_num\", \"follower_count\", \"compound_score\", \"Close_price-today\", \"Close_price-tmr\"]]\n",
    "y_test = df_test['Close_price-2days']\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "test_score = knn_reg.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"Test score on the fifth dataset:\", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e058235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "filepaths = [    \"Ultimate_AAPL_filtered.csv\",    \"Ultimate_TSLA_filtered.csv\",    \"Ultimate_MSFT_filtered.csv\",    \"Ultimate_GOOG_filtered.csv\",    \"Ultimate_AMZN_filtered.csv\",]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "knn_reg = KNeighborsRegressor()\n",
    "\n",
    "# Train the model using the first four datasets\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for filepath in filepaths[:-1]:\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    X = df[[\"comment_num\", \"retweet_num\", \"like_num\", \"follower_count\", \"compound_score\", \"Close_price-today\", \"Close_price-tmr\"]]\n",
    "    y = df['Close_price-2days']\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_train_list.append(X_scaled)\n",
    "    y_train_list.append(y.values)\n",
    "\n",
    "X_train = np.vstack(X_train_list)\n",
    "y_train = np.concatenate(y_train_list)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "params = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "grid = GridSearchCV(knn_reg, params, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "knn_reg_best = grid.best_estimator_\n",
    "\n",
    "# Ensemble methods using BaggingRegressor\n",
    "bagging_reg = BaggingRegressor(base_estimator=knn_reg_best, n_estimators=10, random_state=0)\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the fifth dataset\n",
    "df_test = pd.read_csv(filepaths[-1])\n",
    "\n",
    "X_test = df_test[[\"comment_num\", \"retweet_num\", \"like_num\", \"follower_count\", \"compound_score\", \"Close_price-today\", \"Close_price-tmr\"]]\n",
    "y_test = df_test['Close_price-2days']\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "test_score = bagging_reg.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"Test score on the fifth dataset:\", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb54c6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "params = {\n",
    "    'knn__n_neighbors': range(3, 11),\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__p': [1, 2],\n",
    "    'knn__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "    'selectkbest__k': range(3, 8)\n",
    "}\n",
    "\n",
    "knn_reg = KNeighborsRegressor()\n",
    "selectkbest = SelectKBest(f_regression)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('selectkbest', selectkbest),\n",
    "    ('knn', knn_reg)\n",
    "])\n",
    "\n",
    "# Use RandomizedSearchCV instead of GridSearchCV\n",
    "random_search = RandomizedSearchCV(pipe, params, n_iter=50, cv=5, n_jobs=-1, verbose=1, random_state=42)\n",
    "print(\"Starting random search...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Random search complete.\")\n",
    "\n",
    "knn_reg_best = random_search.best_estimator_\n",
    "print(\"Best estimator found:\", knn_reg_best)\n",
    "\n",
    "bagging_reg = BaggingRegressor(base_estimator=knn_reg_best, n_estimators=10, random_state=0)\n",
    "print(\"Fitting bagging regressor...\")\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "print(\"Bagging regressor fit complete.\")\n",
    "\n",
    "df_test = pd.read_csv(filepaths[-1])\n",
    "X_test = df_test[[\"comment_num\", \"retweet_num\", \"like_num\", \"follower_count\", \"compound_score\", \"Close_price-today\", \"Close_price-tmr\"]]\n",
    "y_test = df_test['Close_price-2days']\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "test_score = bagging_reg.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"Test score on the fifth dataset:\", test_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
