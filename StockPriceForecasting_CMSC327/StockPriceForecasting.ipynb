{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916bdb33",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac051501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#read file.\n",
    "df1 = pd.read_csv('Ultimate_AAPL_filtered.csv')\n",
    "df2 = pd.read_csv('Ultimate_AMZN_filtered.csv')\n",
    "df3 = pd.read_csv('Ultimate_GOOGL_filtered.csv')\n",
    "df4 = pd.read_csv('Ultimate_MSFT_filtered.csv')\n",
    "df5 = pd.read_csv('Ultimate_TSLA_filtered.csv')\n",
    "\n",
    "# combine all companies' data\n",
    "df = pd.DataFrame()\n",
    "df = df.append(df1, ignore_index=True)\n",
    "df = df.append(df2, ignore_index=True)\n",
    "df = df.append(df3, ignore_index=True)\n",
    "df = df.append(df4, ignore_index=True)\n",
    "df = df.append(df5, ignore_index=True)\n",
    "df= df.drop(columns=['post_date','post_date_date'])\n",
    "df['Increase'] = (df['Close_price-2days'] > df['Close_price-tmr'])\n",
    "X=df[[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "Y=df['Increase']\n",
    "\n",
    "    \n",
    "# normalize variables\n",
    "scaler = MinMaxScaler()\n",
    "scaledX = scaler.fit_transform(X)\n",
    "scaled_df = pd.DataFrame(scaledX,columns=[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"])\n",
    "scaled_df['Increase'] = Y\n",
    "newX=scaled_df[[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "newY=scaled_df['Increase']\n",
    "\n",
    "# split training data and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(newX, newY, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_list = [1,10,100,1000]\n",
    "scores_linear = []\n",
    "scores_poly = []\n",
    "scores_gauss = []\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "for i in range(len(C_list)):\n",
    "    svm_linear = SVC(kernel=\"linear\", C=C_list[i])\n",
    "    svm_poly = SVC(kernel='poly', degree=2, C=C_list[i])  #polynomial kernel with degree 2.\n",
    "    svm_gauss = SVC(kernel='rbf', gamma=1, C=C_list[i])   #gaussian rbf kernel\n",
    "    \n",
    "    #train the model with train data set\n",
    "    svm_linear.fit(X_train,y_train) \n",
    "    svm_poly.fit(X_train,y_train)\n",
    "    svm_gauss.fit(X_train,y_train)\n",
    "    \n",
    "    #test the model with test data set and see how close are the prediction from real\n",
    "    scores_linear.append(svm_linear.score(X_test,y_test))\n",
    "    scores_poly.append(svm_poly.score(X_test,y_test))\n",
    "    scores_gauss.append(svm_gauss.score(X_test,y_test))\n",
    "\n",
    "scores = pd.DataFrame([scores_linear,scores_poly,scores_gauss], index = ['linear','poly','gauss'], columns = C_list)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af05aa91",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da28df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#read file.\n",
    "df1 = pd.read_csv('Ultimate_AAPL_filtered.csv')\n",
    "df2 = pd.read_csv('Ultimate_AMZN_filtered.csv')\n",
    "df3 = pd.read_csv('Ultimate_GOOGL_filtered.csv')\n",
    "df4 = pd.read_csv('Ultimate_MSFT_filtered.csv')\n",
    "df5 = pd.read_csv('Ultimate_TSLA_filtered.csv')\n",
    "\n",
    "# combine all companies' data\n",
    "df = pd.DataFrame()\n",
    "df = df.append(df1, ignore_index=True)\n",
    "df = df.append(df2, ignore_index=True)\n",
    "df = df.append(df3, ignore_index=True)\n",
    "df = df.append(df4, ignore_index=True)\n",
    "df = df.append(df5, ignore_index=True)\n",
    "df= df.drop(columns=['post_date','post_date_date'])\n",
    "df['Increase'] = (df['Close_price-2days'] > df['Close_price-tmr'])\n",
    "X=df[[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "Y=df['Increase']\n",
    "    \n",
    "# normalize variables\n",
    "scaler = MinMaxScaler()\n",
    "scaledX = scaler.fit_transform(X)\n",
    "scaled_df = pd.DataFrame(scaledX,columns=[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"])\n",
    "scaled_df['Increase'] = Y\n",
    "newX=scaled_df[[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "newY=scaled_df['Increase']\n",
    "    \n",
    "# split training data, validation data, testing data\n",
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(newX, newY, test_size=0.3)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ef94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "#  input shape is 7 since we have 7 input features as indepent variables # all 32 units share the training size\n",
    "# meaning that each unit has size (training_size)/32\n",
    "model = Sequential([\n",
    "    Dense(units = 32, activation='relu', input_shape=(7,)),\n",
    "    Dense(units = 32, activation='relu'),\n",
    "    Dense(units = 1, activation='sigmoid')\n",
    "])\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "#model.compile(loss=BinaryCrossentropy())\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=32, epochs=5, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ab110",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85bbce",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb98672d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#read file.\n",
    "df1 = pd.read_csv('Ultimate_AAPL_filtered.csv')\n",
    "df2 = pd.read_csv('Ultimate_AMZN_filtered.csv')\n",
    "df3 = pd.read_csv('Ultimate_GOOGL_filtered.csv')\n",
    "df4 = pd.read_csv('Ultimate_MSFT_filtered.csv')\n",
    "df5 = pd.read_csv('Ultimate_TSLA_filtered.csv')\n",
    "\n",
    "# combine all companies' data\n",
    "df = pd.DataFrame()\n",
    "df = df.append(df1, ignore_index=True)\n",
    "df = df.append(df2, ignore_index=True)\n",
    "df = df.append(df3, ignore_index=True)\n",
    "df = df.append(df4, ignore_index=True)\n",
    "df = df.append(df5, ignore_index=True)\n",
    "df= df.drop(columns=['post_date','post_date_date'])\n",
    "df['Increase'] = (df['Close_price-2days'] > df['Close_price-tmr'])\n",
    "X=df[[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "Y=df['Increase']\n",
    "    \n",
    "# normalize variables\n",
    "scaler = MinMaxScaler()\n",
    "scaledX = scaler.fit_transform(X)\n",
    "scaled_df = pd.DataFrame(scaledX,columns=[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"])\n",
    "scaled_df['Increase'] = Y\n",
    "newX=scaled_df[[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "newY=scaled_df['Increase']\n",
    "\n",
    "# split training data and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(newX, newY, test_size = 0.30)\n",
    "\n",
    "# set up the logistic regression from sklearn\n",
    "clf = LogisticRegression(random_state=0) # set parameter C=1e12 to make the logistic reg without regulation\n",
    "# train the model with training data X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# generate coefficients' estimations for each variable\n",
    "estimations = pd.DataFrame(clf.coef_.transpose(), [\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]).transpose()\n",
    "estimations = estimations.rename(index = {0:'coeff_estimates'})\n",
    "estimations['intercept'] = clf.intercept_\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "estimations['Accuracy'] = clf.score(X_test,y_test)\n",
    "estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492abd2d",
   "metadata": {},
   "source": [
    "# LSTM: train on Apple, test on Google"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc530f7",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63758a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df1 = pd.read_csv('Ultimate_AAPL_filtered.csv')\n",
    "df3 = pd.read_csv('Ultimate_GOOGL_filtered.csv')\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "plt.title('Stock Prices History for Apple')\n",
    "plt.plot(df1['Close_price-2days'])\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Prices ($)')\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "plt.title('Stock Prices History for Google')\n",
    "plt.plot(df3['Close_price-2days'])\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Prices ($)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d7064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do LSTM on Google in case of the time\n",
    "X=df3[[\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "Y=df3['Close_price-2days']\n",
    "    \n",
    "# normalize variables\n",
    "scaler = MinMaxScaler()\n",
    "scaledX = scaler.fit_transform(X)\n",
    "scaled_df = pd.DataFrame(scaledX,columns=[\"Close_price-today\",\"Close_price-tmr\"])\n",
    "scaled_df['Close_price-2days'] = Y\n",
    "X_train=scaled_df[[\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "Y_train=scaled_df['Close_price-2days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6941a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the LSTM model\n",
    "model = keras.Sequential()\n",
    "model.add(layers.LSTM(100, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(layers.LSTM(100, return_sequences=False))\n",
    "model.add(layers.Dense(25))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "# train the LSTM model using Google's stock\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, Y_train, batch_size= 1, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda8bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on Apple's stock\n",
    "Apple_X=df1[[\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "Apple_Y=df1['Close_price-2days']\n",
    "    \n",
    "# normalize variables\n",
    "scaler = MinMaxScaler()\n",
    "scaledX_Apple = scaler.fit_transform(Apple_X)\n",
    "scaled_df_Apple = pd.DataFrame(scaledX_Apple,columns=[\"Close_price-today\",\"Close_price-tmr\"])\n",
    "scaled_df_Apple['Close_price-2days'] = Apple_Y\n",
    "testX_Apple=scaled_df_Apple[[\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "testY_Apple=scaled_df_Apple['Close_price-2days']\n",
    "\n",
    "predictions = model.predict(testX_Apple)\n",
    "predictions = np.reshape(predictions, (predictions.shape[0])) # get rid of 2nd dimension\n",
    "error = np.sqrt(np.mean(predictions - testY_Apple)**2)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd8c34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Stock Price for Google')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Close Price USD ($)')\n",
    "plt.plot(testY_Apple)\n",
    "plt.plot(predictions)\n",
    "plt.legend(['Real', 'Predict'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282cd23c",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "filepaths = [   \"Ultimate_AAPL_filtered.csv\",    \n",
    "                \"Ultimate_TSLA_filtered.csv\",    \n",
    "                \"Ultimate_MSFT_filtered.csv\",    \n",
    "                \"Ultimate_GOOG_filtered.csv\",    \n",
    "                \"Ultimate_AMZN_filtered.csv\",]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Train the model using the first four datasets\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for filepath in filepaths[:-1]:\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    X = df[[\"comment_num\", \"retweet_num\", \"like_num\", \"follower_count\", \"compound_score\", \"Close_price-today\", \"Close_price-tmr\"]]\n",
    "    y = df['Close_price-2days']\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_train_list.append(X_scaled)\n",
    "    y_train_list.append(y.values)\n",
    "\n",
    "X_train = np.vstack(X_train_list)\n",
    "y_train = np.concatenate(y_train_list)\n",
    "\n",
    "knn_reg.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the fifth dataset\n",
    "df_test = pd.read_csv(filepaths[-1])\n",
    "\n",
    "X_test = df_test[[\"comment_num\", \"retweet_num\", \"like_num\", \"follower_count\", \"compound_score\", \"Close_price-today\", \"Close_price-tmr\"]]\n",
    "y_test = df_test['Close_price-2days']\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "test_score = knn_reg.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"Test score on the fifth dataset:\", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e058235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "filepaths = [    \"Ultimate_AAPL_filtered.csv\",    \"Ultimate_TSLA_filtered.csv\",    \"Ultimate_MSFT_filtered.csv\",    \"Ultimate_GOOG_filtered.csv\",    \"Ultimate_AMZN_filtered.csv\",]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "knn_reg = KNeighborsRegressor()\n",
    "\n",
    "# Train the model using the first four datasets\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for filepath in filepaths[:-1]:\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    X = df[[\"comment_num\", \"retweet_num\", \"like_num\", \"follower_count\", \"compound_score\", \"Close_price-today\", \"Close_price-tmr\"]]\n",
    "    y = df['Close_price-2days']\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_train_list.append(X_scaled)\n",
    "    y_train_list.append(y.values)\n",
    "\n",
    "X_train = np.vstack(X_train_list)\n",
    "y_train = np.concatenate(y_train_list)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "params = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "grid = GridSearchCV(knn_reg, params, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "knn_reg_best = grid.best_estimator_\n",
    "\n",
    "# Ensemble methods using BaggingRegressor\n",
    "bagging_reg = BaggingRegressor(base_estimator=knn_reg_best, n_estimators=10, random_state=0)\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the fifth dataset\n",
    "df_test = pd.read_csv(filepaths[-1])\n",
    "\n",
    "X_test = df_test[[\"comment_num\", \"retweet_num\", \"like_num\", \"follower_count\", \"compound_score\", \"Close_price-today\", \"Close_price-tmr\"]]\n",
    "y_test = df_test['Close_price-2days']\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "test_score = bagging_reg.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"Test score on the fifth dataset:\", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb54c6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "params = {\n",
    "    'knn__n_neighbors': range(3, 11),\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__p': [1, 2],\n",
    "    'knn__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "    'selectkbest__k': range(3, 8)\n",
    "}\n",
    "\n",
    "knn_reg = KNeighborsRegressor()\n",
    "selectkbest = SelectKBest(f_regression)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('selectkbest', selectkbest),\n",
    "    ('knn', knn_reg)\n",
    "])\n",
    "\n",
    "# Use RandomizedSearchCV instead of GridSearchCV\n",
    "random_search = RandomizedSearchCV(pipe, params, n_iter=50, cv=5, n_jobs=-1, verbose=1, random_state=42)\n",
    "print(\"Starting random search...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Random search complete.\")\n",
    "\n",
    "knn_reg_best = random_search.best_estimator_\n",
    "print(\"Best estimator found:\", knn_reg_best)\n",
    "\n",
    "bagging_reg = BaggingRegressor(base_estimator=knn_reg_best, n_estimators=10, random_state=0)\n",
    "print(\"Fitting bagging regressor...\")\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "print(\"Bagging regressor fit complete.\")\n",
    "\n",
    "df_test = pd.read_csv(filepaths[-1])\n",
    "X_test = df_test[[\"comment_num\", \"retweet_num\", \"like_num\", \"follower_count\", \"compound_score\", \"Close_price-today\", \"Close_price-tmr\"]]\n",
    "y_test = df_test['Close_price-2days']\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "test_score = bagging_reg.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"Test score on the fifth dataset:\", test_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7538d78",
   "metadata": {},
   "source": [
    "# Other Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df=pd.read_csv(\"Ultimate_AAPL_filtered.csv\")    #read file.\n",
    "\n",
    "\n",
    "#feature selection \n",
    "X=df[[\"comment_num\", \"retweet_num\", \"like_num\", \"follower_count\", \"compound_score\", \"Close_price-today\", \"Close_price-tmr\"]]#repeat for other columns\n",
    "\n",
    "y = df['Close_price-2days']\n",
    "\n",
    "X\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature visualization\n",
    "#histogram and cdf for today, tomorrow, 2 days with appropriate bin size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fcb4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from statistics import mean\n",
    "\n",
    "#sample code\n",
    "lin_reg = LinearRegression()\n",
    "np.random.seed(40)  # to make this code example reproducible\n",
    "\n",
    "\n",
    "#kfold = KFold(n_splits=3, shuffle=True, random_state=42) #--->here k=3 with 3-fold cross validation.\n",
    "#random_state controls randomness of each fold.\n",
    "scores = []\n",
    "\"\"\"\n",
    "for k in range(3, 6):\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=42) #--->here k=3,4,5 with k-fold cross validation.\n",
    "    for i, (train, test) in enumerate(kfold.split(X_scaled, y)):#--->splitting into train and test set.\n",
    "        lin_reg.fit(X_scaled[train], y[train])\n",
    "        #Y_pred = regression.predict(X[test])\n",
    "        temp_score = lin_reg.score(X_scaled[test], y[test])\n",
    "        scores.append(temp_score)\n",
    "    print(\"For k =\", k, \" list of results:\")\n",
    "    print(scores)\n",
    "    print(\"For k =\", k, \" the average result is\", mean(scores))\n",
    "    scores = []\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "\n",
    "#     print(i)\n",
    "#     print(train)\n",
    "#     print(test)\n",
    "    \n",
    "#Enter your code here. You can refer lab 1 and assignment 1 document for the implementation of\n",
    "#fit and score.\n",
    "\"\"\"\n",
    "\n",
    "k = 3\n",
    "kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "for i, (train, test) in enumerate(kfold.split(X_scaled, y)):#--->splitting into train and test set.\n",
    "    lin_reg.fit(X_scaled[train], y[train])\n",
    "    #Y_pred = regression.predict(X[test])\n",
    "    temp_score = lin_reg.score(X_scaled[test], y[test])\n",
    "    scores.append(temp_score)\n",
    "print(\"For k =\", k, \" list of results:\")\n",
    "print(scores)\n",
    "print(\"For k =\", k, \" the average result is\", mean(scores))\n",
    "scores = []\n",
    "print(\" \")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f965b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "gamma_reg = linear_model.GammaRegressor()\n",
    "\n",
    "k = 3\n",
    "kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "for i, (train, test) in enumerate(kfold.split(X_scaled, y)):#--->splitting into train and test set.\n",
    "    gamma_reg.fit(X_scaled[train], y[train])\n",
    "    #Y_pred = regression.predict(X[test])\n",
    "    temp_score = gamma_reg.score(X_scaled[test], y[test])\n",
    "    scores.append(temp_score)\n",
    "print(\"For k =\", k, \" list of results:\")\n",
    "print(scores)\n",
    "print(\"For k =\", k, \" the average result is\", mean(scores))\n",
    "scores = []\n",
    "print(\" \")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a0ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#bayes_reg = linear_model.BayesianRidge()\n",
    "scores=[]\n",
    "#lin_reg = LinearRegression()\n",
    "for i, (train, test) in enumerate(kfold.split(X_scaled, y)):#--->splitting into train and test set.\n",
    "    #bayes_reg.fit(X_scaled[train], y[train])\n",
    "    #Y_pred = regression.predict(X[test])\n",
    "    temp_score = lin_reg.score(X_scaled[test], y[test])\n",
    "    scores.append(temp_score)\n",
    "print(\"For k =\", k, \" list of results:\")\n",
    "print(scores)\n",
    "print(\"For k =\", k, \" the average result is\", mean(scores))\n",
    "scores = []\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a83399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\"\"\"\n",
    "length_data = len(data)     # rows that data has\n",
    "split_ratio = 0.7           # %70 train + %30 validation\n",
    "length_train = round(length_data * split_ratio)  \n",
    "length_validation = length_data - length_train\n",
    "print(\"Data length :\", length_data)\n",
    "print(\"Train data length :\", length_train)\n",
    "print(\"Validation data lenth :\", length_validation)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#####\n",
    "\n",
    "X_train, X_test,\n",
    "y_train, y_test = train_test_split(X,y ,\n",
    "                                   random_state=104, \n",
    "                                   test_size=0.25, \n",
    "                                   shuffle=True)\n",
    "\n",
    "\n",
    "# initializing the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# adding first RNN layer and dropout regulatization\n",
    "regressor.add(\n",
    "    SimpleRNN(units = 50, \n",
    "              activation = \"tanh\", \n",
    "              return_sequences = True, \n",
    "              input_shape = (X_train.shape[1],1))\n",
    "             )\n",
    "\n",
    "regressor.add(\n",
    "    Dropout(0.2)\n",
    "             )\n",
    "\n",
    "regressor.add(\n",
    "    SimpleRNN(units = 50, \n",
    "              activation = \"tanh\", \n",
    "              return_sequences = True)\n",
    "             )\n",
    "\n",
    "regressor.add(\n",
    "    Dropout(0.2)\n",
    "             )\n",
    "\n",
    "# adding third RNN layer and dropout regulatization\n",
    "\n",
    "regressor.add(\n",
    "    SimpleRNN(units = 50, \n",
    "              activation = \"tanh\", \n",
    "              return_sequences = True)\n",
    "             )\n",
    "\n",
    "\n",
    "regressor.add(\n",
    "    Dropout(0.2)\n",
    "             )\n",
    "\n",
    "# adding fourth RNN layer and dropout regulatization\n",
    "\n",
    "regressor.add(\n",
    "    SimpleRNN(units = 50)\n",
    "             )\n",
    "\n",
    "regressor.add(\n",
    "    Dropout(0.2)\n",
    "             )\n",
    "\n",
    "# adding the output layer\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "# compiling RNN\n",
    "regressor.compile(\n",
    "    optimizer = \"adam\", \n",
    "    loss = \"mean_squared_error\",\n",
    "    metrics = [\"accuracy\"])\n",
    "\n",
    "\n",
    "# fitting the RNN\n",
    "history = regressor.fit(X_train, y_train, epochs = 50, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "filepaths = [\n",
    "    \"Ultimate_AAPL_filtered.csv\",\n",
    "    \"Ultimate_TSLA_filtered.csv\",\n",
    "    \"Ultimate_MSFT_filtered.csv\",\n",
    "    \"Ultimate_GOOG_filtered.csv\",\n",
    "    \"Ultimate_AMZN_filtered.csv\",\n",
    "]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Train the model using the first four datasets\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for filepath in filepaths[:-1]:\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    X = df[[\"comment_num\", \"retweet_num\", \"like_num\", \"follower_count\", \"compound_score\", \"Close_price-today\", \"Close_price-tmr\"]]\n",
    "    y = df['Close_price-2days']\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_train_list.append(X_scaled)\n",
    "    y_train_list.append(y.values)\n",
    "\n",
    "X_train = np.vstack(X_train_list)\n",
    "y_train = np.concatenate(y_train_list)\n",
    "\n",
    "\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the fifth dataset\n",
    "df_test = pd.read_csv(filepaths[-1])\n",
    "\n",
    "X_test = df_test[[\"comment_num\", \"retweet_num\", \"like_num\", \"follower_count\", \"compound_score\", \"Close_price-today\", \"Close_price-tmr\"]]\n",
    "y_test = df_test['Close_price-2days']\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "test_score = lin_reg.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"Test score on the fifth dataset:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import linear_model\n",
    "\n",
    "filepaths = [\n",
    "    \"Ultimate_AAPL_filtered.csv\",\n",
    "    \"Ultimate_TSLA_filtered.csv\",\n",
    "    \"Ultimate_MSFT_filtered.csv\",\n",
    "    \"Ultimate_GOOG_filtered.csv\",\n",
    "    \"Ultimate_AMZN_filtered.csv\",\n",
    "]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "#lin_reg = LinearRegression()\n",
    "gamma_reg = linear_model.GammaRegressor()\n",
    "\n",
    "# Train the model using the first four datasets\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for filepath in filepaths[:-1]:\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    X = df[[\"comment_num\", \"retweet_num\", \"like_num\", \"follower_count\", \"compound_score\", \"Close_price-today\", \"Close_price-tmr\"]]\n",
    "    y = df['Close_price-2days']\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_train_list.append(X_scaled)\n",
    "    y_train_list.append(y.values)\n",
    "\n",
    "X_train = np.vstack(X_train_list)\n",
    "y_train = np.concatenate(y_train_list)\n",
    "\n",
    "\n",
    "gamma_reg.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the fifth dataset\n",
    "df_test = pd.read_csv(filepaths[-1])\n",
    "\n",
    "\n",
    "X_test = df_test[[\"comment_num\", \"retweet_num\", \"like_num\", \"follower_count\", \"compound_score\", \"Close_price-today\", \"Close_price-tmr\"]]\n",
    "y_test = df_test['Close_price-2days']\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "test_score = gamma_reg.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"Test score on the fifth dataset:\", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8088da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Ultimate_AAPL_filtered.csv\")\n",
    "stock = df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c29c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_change(x,period=1):\n",
    "    x = np.array(x)\n",
    "    return ((x[period:] - x[:-period]) / x[:-period])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1177a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_simulation = 100\n",
    "predict_day = 30\n",
    "returns = df['Close_price-today'].pct_change()\n",
    "volatility = returns.std()\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for i in range(number_simulation):\n",
    "    prices = []\n",
    "    prices.append(df['Close_price-today'].iloc[-1])\n",
    "    for d in range(predict_day):\n",
    "        prices.append(prices[d] * (1 + np.random.normal(0, volatility)))\n",
    "    results[i] = pd.Series(prices).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdfda5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(results)\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Simulated days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a32de",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b076a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Ultimate_AAPL_filtered.csv\")\n",
    "df['change_in_price'] = (df['Close_price-2days'] - df['Close_price-tmr'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a65ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = df[\"compound_score\"]\n",
    "Y = df[\"change_in_price\"]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(X, Y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e6b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "df = pd.read_csv(\"Ultimate_TSLA_filtered.csv\")\n",
    "df['change_in_price'] = (df['Close_price-2days'] - df['Close_price-tmr'])\n",
    "\n",
    "X = df[\"compound_score\"]\n",
    "Y = df[\"change_in_price\"]\n",
    "\n",
    "corr = pearsonr(X, Y)[0]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae334b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"Ultimate_TSLA_filtered.csv\")\n",
    "\n",
    "X = df[\"Close_price-today\"]\n",
    "Y = df[\"Close_price-tmr\"]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e37b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "df = pd.read_csv(\"Ultimate_TSLA_filtered.csv\")\n",
    "df['change_in_price'] = (df['Close_price-2days'] - df['Close_price-tmr'])\n",
    "\n",
    "X = df[\"Close_price-today\"]\n",
    "Y = df[\"Close_price-tmr\"]\n",
    "\n",
    "corr = pearsonr(X, Y)[0]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd662ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Pearson correlation\n",
    "df = pd.read_csv(\"Ultimate_TSLA_filtered.csv\")\n",
    "df['change_in_price'] = (df['Close_price-2days'] - df['Close_price-tmr'])\n",
    "\n",
    "X = df[\"compound_score\"]\n",
    "Y = df[\"change_in_price\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b893de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = pd.read_csv('Ultimate_AAPL_filtered.csv')\n",
    "df2 = pd.read_csv('Ultimate_AMZN_filtered.csv')\n",
    "df3 = pd.read_csv('Ultimate_GOOGL_filtered.csv')\n",
    "df4 = pd.read_csv('Ultimate_MSFT_filtered.csv')\n",
    "\n",
    "# combine all companies' data\n",
    "df = pd.DataFrame()\n",
    "df = df.append(df1, ignore_index=True)\n",
    "df = df.append(df2, ignore_index=True)\n",
    "df = df.append(df3, ignore_index=True)\n",
    "df = df.append(df4, ignore_index=True)\n",
    "df= df.drop(columns=['post_date','post_date_date'])\n",
    "df['Increase'] = (df['Close_price-2days'] > df['Close_price-tmr'])\n",
    "X=df[[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "Y=df['Increase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0da3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "LDA.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada5b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('Ultimate_TSLA_filtered.csv')\n",
    "df5['Increase'] = (df5['Close_price-2days'] > df5['Close_price-tmr'])\n",
    "X=df5[[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "Y=df5['Increase']\n",
    "LDA.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e14ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Ultimate_AAPL_filtered.csv')\n",
    "df2 = pd.read_csv('Ultimate_AMZN_filtered.csv')\n",
    "df3 = pd.read_csv('Ultimate_GOOGL_filtered.csv')\n",
    "df4 = pd.read_csv('Ultimate_MSFT_filtered.csv')\n",
    "\n",
    "# combine all companies' data\n",
    "df = pd.DataFrame()\n",
    "df = df.append(df1, ignore_index=True)\n",
    "df = df.append(df2, ignore_index=True)\n",
    "df = df.append(df3, ignore_index=True)\n",
    "df = df.append(df4, ignore_index=True)\n",
    "df= df.drop(columns=['post_date','post_date_date'])\n",
    "df['Increase'] = (df['Close_price-2days'] > df['Close_price-tmr'])\n",
    "X=df[[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "Y=df['Increase']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9eefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "QDA = QuadraticDiscriminantAnalysis()\n",
    "QDA.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('Ultimate_TSLA_filtered.csv')\n",
    "df5['Increase'] = (df5['Close_price-2days'] > df5['Close_price-tmr'])\n",
    "X=df5[[\"comment_num\",\"retweet_num\",\"like_num\", \"follower_count\",\"compound_score\",\"Close_price-today\",\"Close_price-tmr\"]]\n",
    "Y=df5['Increase']\n",
    "QDA.score(X, Y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
